{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Networks vs Recurrent Neural Networks\n",
    "\n",
    "A feed-forward neural network allows information to flow only in the forward direction, from the input nodes, through the hidden layers, and to the output nodes. There are no cycles or loops in the network. \n",
    "\n",
    "In a feed-forward neural network, the decisions are based on the current input. It doesn’t memorize the past data, and there’s no future scope. Feed-forward neural networks are used in general regression and classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RNN**\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of artificial neural network designed to process sequences of data. They work especially well for jobs requiring sequences, such as time series data, voice, natural language, and other activities.\n",
    "\n",
    "RNN works on the principle of saving the output of a particular layer and feeding this back to the input in order to predict the output of the layer.\n",
    "\n",
    "Below is how you can convert a Feed-Forward Neural Network into a Recurrent Neural Network:\n",
    "\n",
    "<img src=\"images/nn2rnn.gif\" width=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReccurentNN:\n",
    "    def __init__(self, char_to_idx, idx_to_char, vocab, h_size=75,\n",
    "                seq_len=20, clip_value=5, epochs=50, learning_rate=1e-2):\n",
    "        self.n_h = h_size \n",
    "        self.seq_len = seq_len \n",
    "        self.clip_value = clip_value  \n",
    "        self.epochs = epochs  \n",
    "        self.learning_rate = learning_rate\n",
    "        self.char_to_idx = char_to_idx  \n",
    "        self.idx_to_char = idx_to_char  \n",
    "        self.vocab = vocab  \n",
    "\n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab) * self.seq_len  \n",
    "\n",
    "        # -----initialise weights and biases----- #\n",
    "        self.params = {}\n",
    "        self.params[\"W_xh\"] = np.random.randn(self.vocab, self.n_h) * 0.01 \n",
    "        self.params[\"W_hh\"] = np.identity(self.n_h) * 0.01\n",
    "        self.params[\"b_h\"] = np.zeros((1, self.n_h))\n",
    "        self.params[\"W_hy\"] = np.random.randn(self.n_h, self.vocab) * 0.01\n",
    "        self.params[\"b_y\"] = np.zeros((1, self.vocab))\n",
    "        self.h0 = np.zeros((1, self.n_h))  # value of the hidden state at time step t = -1\n",
    "\n",
    "        # -----initialise gradients and memory parameters for Adagrad ----- # \n",
    "        self.grads = {}\n",
    "        self.m_params = {}\n",
    "        for key in self.params:\n",
    "            self.grads[\"d\" + key] = np.zeros_like(self.params[key])\n",
    "            self.m_params[\"m\" + key] = np.zeros_like(self.params[key])\n",
    "\n",
    "    def _encode_text(self, X):\n",
    "        X_encoded = []\n",
    "        for char in X:\n",
    "            X_encoded.append(self.char_to_idx[char])\n",
    "        return X_encoded\n",
    "\n",
    "    def _prepare_batches(self, X, index):\n",
    "            X_batch_encoded = X[index: index + self.seq_len]\n",
    "            y_batch_encoded = X[index + 1: index + self.seq_len + 1]\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in X_batch_encoded:\n",
    "                one_hot_char = np.zeros((1, self.vocab))\n",
    "                one_hot_char[0][i] = 1\n",
    "                X_batch.append(one_hot_char)\n",
    "            for j in y_batch_encoded:\n",
    "                one_hot_char = np.zeros((1, self.vocab))\n",
    "                one_hot_char[0][j] = 1\n",
    "                y_batch.append(one_hot_char)\n",
    "            return X_batch, y_batch \n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x)) \n",
    "        return e_x / np.sum(e_x)\n",
    "    \n",
    "    def _forward_pass(self, X):\n",
    "        h = {}  \n",
    "        h[-1] = self.h0  # set initial hidden state at t=-1\n",
    "        y_pred = {}  # stores softmax output probabilities\n",
    "\n",
    "        # iterate over each character in the input sequence\n",
    "        for t in range(self.seq_len):\n",
    "            h[t] = np.tanh(\n",
    "                np.dot(X[t], self.params[\"W_xh\"]) + np.dot(h[t - 1], self.params[\"W_hh\"]) + self.params[\"b_h\"])\n",
    "            y_pred[t] = self._softmax(np.dot(h[t], self.params[\"W_hy\"]) + self.params[\"b_y\"])\n",
    "        self.ho = h[t]\n",
    "        return y_pred, h\n",
    "    \n",
    "    def _backward_pass(self, X, y, y_pred, h):\n",
    "        dh_next = np.zeros_like(h[0])\n",
    "\n",
    "        for t in reversed(range(self.seq_len)):\n",
    "            dy = np.copy(y_pred[t])\n",
    "            dy[0][np.argmax(y[t])] -= 1  # predicted y - actual y\n",
    "            self.grads[\"dW_hy\"] += np.dot(h[t].T, dy)\n",
    "            self.grads[\"db_y\"] += dy\n",
    "            dhidden = (1 - h[t] ** 2) * (np.dot(dy, self.params[\"W_hy\"].T) + dh_next)\n",
    "            dh_next = np.dot(dhidden, self.params[\"W_hh\"].T)\n",
    "            self.grads[\"dW_hh\"] += np.dot(h[t - 1].T, dhidden)\n",
    "            self.grads[\"dW_xh\"] += np.dot(X[t].T, dhidden)\n",
    "            self.grads[\"db_h\"] += dhidden\n",
    "\n",
    "        for grad, key in enumerate(self.grads):\n",
    "            np.clip(self.grads[key], -self.clip_value, self.clip_value, out=self.grads[key])\n",
    "        return\n",
    "    \n",
    "    def _update(self):\n",
    "        for key in self.params:\n",
    "            self.m_params[\"m\" + key] += self.grads[\"d\" + key] * self.grads[\"d\" + key]\n",
    "            self.params[key] -= self.grads[\"d\" + key] * self.learning_rate / (np.sqrt(self.m_params[\"m\" + key]) + 1e-8) \n",
    "  \n",
    "    def train(self, X):\n",
    "        loss = []\n",
    "\n",
    "        num_batches = len(X) // self.seq_len\n",
    "        X_trimmed = X[:num_batches * self.seq_len]\n",
    "\n",
    "        X_encoded = self._encode_text(X_trimmed)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(0, len(X_encoded) - self.seq_len, self.seq_len):\n",
    "                X_batch, y_batch = self._prepare_batches(X_encoded, j)\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred, h = self._forward_pass(X_batch)\n",
    "\n",
    "                # loss calculation\n",
    "                loss_batch = 0\n",
    "                for t in range(self.seq_len):\n",
    "                    loss_batch += -np.log(y_pred[t][0, np.argmax(y_batch[t])])\n",
    "\n",
    "                # Update smooth loss and append to loss list\n",
    "                self.smooth_loss = self.smooth_loss * 0.999 + loss_batch * 0.001\n",
    "                loss.append(self.smooth_loss)\n",
    "\n",
    "                # Backward pass and update parameters\n",
    "                self._backward_pass(X_batch, y_batch, y_pred, h)\n",
    "                self._update()\n",
    "\n",
    "            # Print training progress\n",
    "            print(f'Epoch: {i + 1}\\tLoss: {loss[-1]}')\n",
    "            print(self.test(50, 2))\n",
    "        return loss, self.params\n",
    "\n",
    "    def test(self, test_size, start_index):\n",
    "        res = \"\"\n",
    "        x = np.zeros((1, self.vocab))\n",
    "        x[0][start_index] = 1\n",
    "        for i in range(test_size):\n",
    "            # forward propagation\n",
    "            h = np.tanh(np.dot(x, self.params[\"W_xh\"]) + np.dot(self.h0, self.params[\"W_hh\"]) + self.params[\"b_h\"])\n",
    "            y_pred = self._softmax(np.dot(h, self.params[\"W_hy\"]) + self.params[\"b_y\"])\n",
    "            # get a random index from the probability distribution of y\n",
    "            index = np.random.choice(range(self.vocab), p=y_pred.ravel())\n",
    "            # set x-one_hot_vector for the next character\n",
    "            x = np.zeros((1, self.vocab))\n",
    "            x[0][index] = 1\n",
    "            # find the char with the index and concat to the output string\n",
    "            char = self.idx_to_char[index]\n",
    "            res += char\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HarryPotter_AllBooks.txt') as file:\n",
    "    text = file.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only a part of the text to make the process faster\n",
    "text = text[:50000] \n",
    "chars = set(text)\n",
    "vocab = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the encoding decoding dictionaries\n",
    "char_to_idx = {w: i for i, w in enumerate(chars)}\n",
    "idx_to_char = {i: w for i, w in enumerate(chars)}\n",
    "rnnParameters = {\n",
    "        'epochs': 10,\n",
    "        'char_to_idx': char_to_idx,\n",
    "        'idx_to_char': idx_to_char,\n",
    "        'vocab': vocab,\n",
    "        'h_size': 75,\n",
    "        'seq_len': 20,  # keeping small to avoid diminishing/exploding gradients\n",
    "        'clip_value': 5,\n",
    "        'learning_rate': 1e-2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does Recurrent Neural Networks Work?\n",
    "\n",
    "In Recurrent Neural networks, the information cycles through a loop to the middle hidden layer.\n",
    "\n",
    "<img src=\"images/rnn.gif\" width=500/>\n",
    "\n",
    "The input layer ‘x’ takes in the input to the neural network and processes it and passes it onto the middle layer. \n",
    "\n",
    "The middle layer ‘h’ can consist of multiple hidden layers, each with its own activation functions and weights and biases. If you have a neural network where the various parameters of different hidden layers are not affected by the previous layer, ie: the neural network does not have memory, then you can use a recurrent neural network.\n",
    "\n",
    "The Recurrent Neural Network will standardize the different activation functions and weights and biases so that each hidden layer has the same parameters. Then, instead of creating multiple hidden layers, it will create one and loop over it as many times as required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tLoss: 52.32316888461033\n",
      "hind bs is ptnthorey thert s tr asthord ved s .co \n",
      "Epoch: 2\tLoss: 48.093867837723565\n",
      "b aulappe cidy .nren pt wirsatd thikedecaavee hick\n",
      "Epoch: 3\tLoss: 46.8850690476119\n",
      "pqudnisd lid or a .momole ike tt wrey slt ots ver \n",
      "Epoch: 4\tLoss: 46.35404033768812\n",
      "thed .itice onccareming py sf t ane .d budavecsme \n",
      "Epoch: 5\tLoss: 46.04717780354482\n",
      "tid lsad ny ls winounon altle t a f sastton tlleck\n",
      "Epoch: 6\tLoss: 45.8411466304117\n",
      "tomyt f l ines alt s lifided ad ifr at satirry lar\n",
      "Epoch: 7\tLoss: 45.67792053455727\n",
      "the upe mce heyle ?t cko ambinonomond .heravetheys\n",
      "Epoch: 8\tLoss: 45.56331430772447\n",
      "t wixkint therang p .he d fut ne sacklit oupevery \n",
      "Epoch: 9\tLoss: 45.471320088592115\n",
      "ingndonit .doke cadase .dotconivy fay wraikey was \n",
      "Epoch: 10\tLoss: 45.40079313050033\n",
      "wanat s lchedin ade n ladat canale oustonad whel t\n"
     ]
    }
   ],
   "source": [
    "RNN = ReccurentNN(**rnnParameters)\n",
    "rnnLoss, params = RNN.train(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### The main limitation of RNNs is that RNNs can’t remember very long sequences and get into the problem of vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LSTM**\n",
    "\n",
    "LSTMs come to the rescue to solve the vanishing gradient problem. It does so by ignoring (forgetting) useless data/information in the network. The LSTM will forget the data if there is no useful information from other inputs (prior sentence words). When new information comes, the network determines which information to be overlooked and which to be remembered.\n",
    "\n",
    "LSTMs have 4 different components, namely\n",
    "Cell state (Memory cell)\n",
    "Forget gate\n",
    "Input gate\n",
    "Output gate\n",
    "\n",
    "<img src=\"images/lstmArchitecture.jpeg\" width=500/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnnLSTM:\n",
    "    def __init__(self, char_to_idx, idx_to_char, epochs=10, vocab=20, n_h=100, \n",
    "                 seq_len=25, learning_rate=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.char_to_idx = char_to_idx \n",
    "        self.idx_to_char = idx_to_char  \n",
    "        self.vocab_size = vocab \n",
    "        self.n_h = n_h \n",
    "        self.seq_len = seq_len  \n",
    "        self.epochs = epochs  \n",
    "        self.lr = learning_rate  \n",
    "        self.beta1 = beta1  \n",
    "        self.beta2 = beta2  \n",
    "\n",
    "        # -----initialise weights and biases----- #\n",
    "        self.params = {}\n",
    "        std = (1.0 / np.sqrt(self.vocab_size + self.n_h))  # Xavier initialisation\n",
    "\n",
    "        # forget gate\n",
    "        self.params[\"Wf\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bf\"] = np.ones((self.n_h, 1))\n",
    "        # input gate\n",
    "        self.params[\"Wi\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bi\"] = np.zeros((self.n_h, 1))\n",
    "        # cell gate\n",
    "        self.params[\"Wc\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bc\"] = np.zeros((self.n_h, 1))\n",
    "        # output gate\n",
    "        self.params[\"Wo\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bo\"] = np.zeros((self.n_h, 1))\n",
    "        # output\n",
    "        self.params[\"Wv\"] = np.random.randn(self.vocab_size, self.n_h) * \\\n",
    "                            (1.0 / np.sqrt(self.vocab_size))\n",
    "        self.params[\"bv\"] = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "        # -----initialise gradients and Adam parameters----- #\n",
    "        self.grads = {}\n",
    "        self.adam_params = {}\n",
    "\n",
    "        for key in self.params:\n",
    "            self.grads[\"d\" + key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"m\" + key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"v\" + key] = np.zeros_like(self.params[key])\n",
    "\n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab_size) * self.seq_len\n",
    "        return\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))  # max(x) subtracted for numerical stability\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "    def clip_grads(self):\n",
    "        for key in self.grads:\n",
    "            np.clip(self.grads[key], -5, 5, out=self.grads[key])\n",
    "        return\n",
    "\n",
    "    def reset_grads(self):\n",
    "        for key in self.grads:\n",
    "            self.grads[key].fill(0)\n",
    "        return\n",
    "\n",
    "    def update_params(self, batch_num):\n",
    "        for key in self.params:\n",
    "            self.adam_params[\"m\" + key] = self.adam_params[\"m\" + key] * self.beta1 + \\\n",
    "                                          (1 - self.beta1) * self.grads[\"d\" + key]\n",
    "            self.adam_params[\"v\" + key] = self.adam_params[\"v\" + key] * self.beta2 + \\\n",
    "                                          (1 - self.beta2) * self.grads[\"d\" + key] ** 2\n",
    "\n",
    "            m_correlated = self.adam_params[\"m\" + key] / (1 - self.beta1 ** batch_num)\n",
    "            v_correlated = self.adam_params[\"v\" + key] / (1 - self.beta2 ** batch_num)\n",
    "            self.params[key] -= self.lr * m_correlated / (np.sqrt(v_correlated) + 1e-8)\n",
    "        return\n",
    "\n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        z = np.row_stack((h_prev, x))\n",
    "\n",
    "        f = self.sigmoid(np.dot(self.params[\"Wf\"], z) + self.params[\"bf\"])\n",
    "        i = self.sigmoid(np.dot(self.params[\"Wi\"], z) + self.params[\"bi\"])\n",
    "        c_bar = np.tanh(np.dot(self.params[\"Wc\"], z) + self.params[\"bc\"])\n",
    "\n",
    "        c = f * c_prev + i * c_bar\n",
    "        o = self.sigmoid(np.dot(self.params[\"Wo\"], z) + self.params[\"bo\"])\n",
    "        h = o * np.tanh(c)\n",
    "\n",
    "        v = np.dot(self.params[\"Wv\"], h) + self.params[\"bv\"]\n",
    "        y_hat = self.softmax(v)\n",
    "        return y_hat, v, h, o, c, c_bar, i, f, z\n",
    "\n",
    "    def backward_step(self, y, y_hat, dh_next, dc_next, c_prev, z, f, i, c_bar, c, o, h):\n",
    "        dv = np.copy(y_hat)\n",
    "        dv[y] -= 1  # yhat - y\n",
    "\n",
    "        self.grads[\"dWv\"] += np.dot(dv, h.T)\n",
    "        self.grads[\"dbv\"] += dv\n",
    "\n",
    "        dh = np.dot(self.params[\"Wv\"].T, dv)\n",
    "        dh += dh_next\n",
    "\n",
    "        do = dh * np.tanh(c)\n",
    "        da_o =         do * o * (1 - o)\n",
    "        self.grads[\"dWo\"] += np.dot(da_o, z.T)\n",
    "        self.grads[\"dbo\"] += da_o\n",
    "\n",
    "        dc = dh * o * (1 - np.tanh(c) ** 2)\n",
    "        dc += dc_next\n",
    "\n",
    "        dc_bar = dc * i\n",
    "        da_c = dc_bar * (1 - c_bar ** 2)\n",
    "        self.grads[\"dWc\"] += np.dot(da_c, z.T)\n",
    "        self.grads[\"dbc\"] += da_c\n",
    "\n",
    "        di = dc * c_bar\n",
    "        da_i = di * i * (1 - i)\n",
    "        self.grads[\"dWi\"] += np.dot(da_i, z.T)\n",
    "        self.grads[\"dbi\"] += da_i\n",
    "\n",
    "        df = dc * c_prev\n",
    "        da_f = df * f * (1 - f)\n",
    "        self.grads[\"dWf\"] += np.dot(da_f, z.T)\n",
    "        self.grads[\"dbf\"] += da_f\n",
    "\n",
    "        dz = (np.dot(self.params[\"Wf\"].T, da_f)\n",
    "              + np.dot(self.params[\"Wi\"].T, da_i)\n",
    "              + np.dot(self.params[\"Wc\"].T, da_c)\n",
    "              + np.dot(self.params[\"Wo\"].T, da_o))\n",
    "\n",
    "        dh_prev = dz[:self.n_h, :]\n",
    "        dc_prev = f * dc\n",
    "        return dh_prev, dc_prev\n",
    "\n",
    "    def forward_backward(self, x_batch, y_batch, h_prev, c_prev):\n",
    "        x, z = {}, {}\n",
    "        f, i, c_bar, c, o = {}, {}, {}, {}, {}\n",
    "        y_hat, v, h = {}, {}, {}\n",
    "\n",
    "        # Values at t= - 1\n",
    "        h[-1] = h_prev\n",
    "        c[-1] = c_prev\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(self.seq_len):\n",
    "            x[t] = np.zeros((self.vocab_size, 1))\n",
    "            x[t][x_batch[t]] = 1\n",
    "\n",
    "            y_hat[t], v[t], h[t], o[t], c[t], c_bar[t], i[t], f[t], z[t] = \\\n",
    "                self.forward_step(x[t], h[t - 1], c[t - 1])\n",
    "\n",
    "            loss += -np.log(y_hat[t][y_batch[t], 0])\n",
    "\n",
    "        self.reset_grads()\n",
    "\n",
    "        dh_next = np.zeros_like(h[0])\n",
    "        dc_next = np.zeros_like(c[0])\n",
    "\n",
    "        for t in reversed(range(self.seq_len)):\n",
    "            dh_next, dc_next = self.backward_step(y_batch[t], y_hat[t], dh_next,\n",
    "                                                  dc_next, c[t - 1], z[t], f[t], i[t],\n",
    "                                                  c_bar[t], c[t], o[t], h[t])\n",
    "        return loss, h[self.seq_len - 1], c[self.seq_len - 1]\n",
    "    \n",
    "    def sample(self, h_prev, c_prev, sample_size):\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        h = h_prev\n",
    "        c = c_prev\n",
    "        sample_string = \"\"\n",
    "\n",
    "        for t in range(sample_size):\n",
    "            y_hat, _, h, _, c, _, _, _, _ = self.forward_step(x, h, c)\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_hat.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            char = self.idx_to_char[idx]\n",
    "            sample_string += char\n",
    "        return sample_string\n",
    "\n",
    "    def train(self, X, verbose=True):\n",
    "        losses = []  \n",
    "\n",
    "        num_batches = len(X) // self.seq_len\n",
    "        X_trimmed = X[: num_batches * self.seq_len]  # trim input to have full sequences\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            h_prev = np.zeros((self.n_h, 1))\n",
    "            c_prev = np.zeros((self.n_h, 1))\n",
    "\n",
    "            for j in range(0, len(X_trimmed) - self.seq_len, self.seq_len):\n",
    "                # prepare batches\n",
    "                x_batch = [self.char_to_idx[ch] for ch in X_trimmed[j: j + self.seq_len]]\n",
    "                y_batch = [self.char_to_idx[ch] for ch in X_trimmed[j + 1: j + self.seq_len + 1]]\n",
    "\n",
    "                loss, h_prev, c_prev = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
    "\n",
    "                # smooth out loss and store in list\n",
    "                self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
    "                losses.append(self.smooth_loss)\n",
    "\n",
    "                self.clip_grads()\n",
    "\n",
    "                batch_num = epoch * self.epochs + j / self.seq_len + 1\n",
    "                self.update_params(batch_num)\n",
    "\n",
    "                # print out loss and sample string\n",
    "                if verbose:\n",
    "                    if j % 400000 == 0:\n",
    "                        print('Epoch:', epoch+1, '\\tBatch:', j, \"-\", j + self.seq_len,\n",
    "                              '\\tLoss:', round(self.smooth_loss, 2))\n",
    "                        s = self.sample(h_prev, c_prev, sample_size=50)\n",
    "                        print(s, \"\\n\")\n",
    "        return losses, self.params\n",
    "    \n",
    "    def test(self, sample_size=100):\n",
    "        h_prev = np.zeros((self.n_h, 1))\n",
    "        c_prev = np.zeros((self.n_h, 1))\n",
    "        return self.sample(h_prev, c_prev, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the encoding decoding dictionaries\n",
    "char_to_idx = {w: i for i, w in enumerate(chars)}\n",
    "idx_to_char = {i: w for i, w in enumerate(chars)}\n",
    "lstmParameters = {\n",
    "        'epochs': 10,\n",
    "        'char_to_idx': char_to_idx,\n",
    "        'idx_to_char': idx_to_char,\n",
    "        'vocab': vocab,\n",
    "        'learning_rate': 1e-2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LSTM Architecture**\n",
    "\n",
    "In LSTMs, instead of just a simple network with a single activation function, we have multiple components, giving power to the network to forget and remember information.\n",
    "\n",
    "\n",
    "<img src=\"images/lstm.gif\"/>\n",
    "\n",
    "Here's a breakdown of the architecture:\n",
    "\n",
    "**1. Core Component: LSTM Cell**\n",
    "* The core of rnnLSTM is the LSTM cell, which processes sequences of data. \n",
    "* It manages a cell state that can remember information for long periods.\n",
    "* This cell state is updated through gates that control the flow of information.\n",
    "\n",
    "**2. Forget Gate (dWf, dbf)**\n",
    "* This gate decides what information to forget from the previous cell state (c_prev).\n",
    "* It considers both the previous hidden state (h_prev) and the current input (x_t) to make this decision.\n",
    "* The forget gate uses a sigmoid activation function to output values between 0 and 1. \n",
    "    * A value close to 1 indicates keeping the information, while a value close to 0 indicates forgetting it entirely.\n",
    "\n",
    "**3. Input Gate (dWi, dbi)**\n",
    "* This gate controls which new information gets added to the cell state.\n",
    "* It has two parts:\n",
    "    * A sigmoid layer (da_i) that decides which values to update.\n",
    "    * A tanh layer (c_bar) that creates candidate values for the new information.\n",
    "* The forget gate's output and the candidate values are combined to determine what information is actually added to the cell state.\n",
    "\n",
    "**4. Cell State (dc, dWc, dbc)**\n",
    "* This is the core memory unit of the LSTM cell. \n",
    "* It holds the information that is passed through the sequence.\n",
    "* The cell state is updated by combining the forget gate's output with the previous cell state and the input gate's contribution.\n",
    "\n",
    "**5. Output Gate (dWo, dbo)**\n",
    "* This gate controls what information from the cell state is passed on to the next time step's hidden state (h_t).\n",
    "* It uses a sigmoid layer (da_o) to determine which parts of the cell state are relevant for the output.\n",
    "* The cell state is then passed through a tanh function and multiplied by the output gate's output to create the final hidden state.\n",
    "\n",
    "**6. Output Layer (dWv, dbv)**\n",
    "* The final output of the rnLSTM unit (y_hat) is generated by taking the current hidden state (h_t) and projecting it to the vocabulary size using a weights matrix (Wv) and a bias vector (bv).\n",
    "* A softmax function is applied to the output to give a probability distribution over the vocabulary, indicating the likelihood of each character being the next character in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 0 - 25 \tLoss: 86.64\n",
      "ws qgsabipm !mnsyaswnoqrvkeqg3 dqyfpkeuot43?c! ccm \n",
      "\n",
      "Epoch: 2 \tBatch: 0 - 25 \tLoss: 51.79\n",
      "eh thi  wrscuble uncle vernon thes with of letter  \n",
      "\n",
      "Epoch: 3 \tBatch: 0 - 25 \tLoss: 40.99\n",
      "ripbed on the timing touted it it for was ennt lor \n",
      "\n",
      "Epoch: 4 \tBatch: 0 - 25 \tLoss: 37.03\n",
      "en dudley .in his smelting .boary bled goodly .dud \n",
      "\n",
      "Epoch: 5 \tBatch: 0 - 25 \tLoss: 35.03\n",
      "op the some and mafe mo.then .young schoced .sked  \n",
      "\n",
      "Epoch: 6 \tBatch: 0 - 25 \tLoss: 33.77\n",
      "ull notes brown uncle vernon his each bit fach to  \n",
      "\n",
      "Epoch: 7 \tBatch: 0 - 25 \tLoss: 32.97\n",
      "end foring to dorm a tant knew and he miders .harr \n",
      "\n",
      "Epoch: 8 \tBatch: 0 - 25 \tLoss: 32.28\n",
      "re could of that it was spoken his raused down !st \n",
      "\n",
      "Epoch: 9 \tBatch: 0 - 25 \tLoss: 31.86\n",
      "rould .this marge the bay out !she seppees which w \n",
      "\n",
      "Epoch: 10 \tBatch: 0 - 25 \tLoss: 31.56\n",
      "rived it dadgy bedreas .they was pouming ive strai \n",
      "\n"
     ]
    }
   ],
   "source": [
    "LSTM = rnnLSTM(**lstmParameters)\n",
    "lossLSTM, params = LSTM.train(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k tos about .uncle vernon ive she set at on the wink yough he saad speering baysed the time as awnin'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM.test(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toout t anongisple .ne fbeslele waiton bopufomotowey ozanatof my s sryinon t ok f dray f ns s lld te'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN.test(100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [SimpliLearn Deep Learning Tutirials](https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn)\n",
    "2. [Introduction to Long Short-Term Memory (LSTM) - Archit Saxena](https://medium.com/analytics-vidhya/introduction-to-long-short-term-memory-lstm-a8052cd0d4cd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
